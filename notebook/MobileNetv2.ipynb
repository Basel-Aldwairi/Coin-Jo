{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "03c578e4-0c70-414c-9e6a-f4426bff052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8c86751-90f6-4111-b31f-f920abb865bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2671 files belonging to 4 classes.\n",
      "Using 2137 files for training.\n",
      "Using 534 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds = image_dataset_from_directory(\n",
    "    '../Data/proccessed',\n",
    "    batch_size=32,\n",
    "    image_size=(224,224),\n",
    "    subset='both',\n",
    "    seed=18,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d231a8d8-e8c8-44d3-a5bd-36cd8de8bf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "base = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3aa1bb8b-43e7-40f8-afe5-ffe316d30dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(224,224,3))\n",
    "\n",
    "x = preprocess_input(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c559bce3-4569-4ea5-b0a4-c77276c764de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c776ee98-0b37-408f-ba7c-1a12ca631613",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = GlobalAveragePooling2D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "755b02cb-8672-4040-94b3-fb716c120e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(128, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4bb7398-d1eb-452a-b4de-f878cbf61d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = Dense(4, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46974d4f-913b-4907-b413-614830a977c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85c05711-f6b5-4a16-b1ca-1f84d104ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b54c073-fb11-467d-b591-59b68d27c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3, \n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6929307-2e90-429f-8cb8-40d17cc24c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model_ckeckpoint.weights.h5',\n",
    "                             monitor='val_accuracy',\n",
    "                             save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25d15b03-605f-417b-9e9b-322ac7f558ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 402ms/step - accuracy: 0.7178 - loss: 0.6561 - val_accuracy: 0.8034 - val_loss: 0.4791\n",
      "Epoch 2/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 413ms/step - accuracy: 0.8652 - loss: 0.3315 - val_accuracy: 0.8315 - val_loss: 0.3870\n",
      "Epoch 3/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 381ms/step - accuracy: 0.9106 - loss: 0.2406 - val_accuracy: 0.8240 - val_loss: 0.3957\n",
      "Epoch 4/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 380ms/step - accuracy: 0.9209 - loss: 0.2011 - val_accuracy: 0.8446 - val_loss: 0.3561\n",
      "Epoch 5/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 380ms/step - accuracy: 0.9565 - loss: 0.1401 - val_accuracy: 0.8764 - val_loss: 0.3536\n",
      "Epoch 6/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 379ms/step - accuracy: 0.9635 - loss: 0.1245 - val_accuracy: 0.8483 - val_loss: 0.3778\n",
      "Epoch 7/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 385ms/step - accuracy: 0.9789 - loss: 0.0848 - val_accuracy: 0.8708 - val_loss: 0.3441\n",
      "Epoch 8/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 380ms/step - accuracy: 0.9827 - loss: 0.0702 - val_accuracy: 0.8727 - val_loss: 0.3409\n",
      "Epoch 9/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 382ms/step - accuracy: 0.9944 - loss: 0.0485 - val_accuracy: 0.8446 - val_loss: 0.3634\n",
      "Epoch 10/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 381ms/step - accuracy: 0.9939 - loss: 0.0410 - val_accuracy: 0.8727 - val_loss: 0.3523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x75a23daf21b0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=10, \n",
    "          callbacks=[early_stop,checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cd1d413-a2df-4df8-919c-37c7524b9fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 303ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2df9d2aa-5fad-496b-b535-49c83cb0a611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.6816533e-08, 3.7294521e-05, 7.6868492e-01, 2.3127776e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed79e70d-1a1e-4af4-87ed-0e61d686031e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 14:14:27.390908: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for imgs, labels in val_ds.take(1):\n",
    "    img = imgs[0]\n",
    "    label = labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d5dba47-d438-4142-beaa-32e6f77aaf6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m img_ = \u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "img_ = Image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9961fd0d-f2d6-4916-aa42-76b21a80a9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
